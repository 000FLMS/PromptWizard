assistant_llm:
  # put the unique_model_id that you specified in llm_config.yaml
  prompt_opt: az_gpt_4
dir_info:
  # Base directory in which every generated file would be saved.
  base_dir: /home/user/VeLLMPlatform
  # Directory inside which system log files should be saved
  log_dir_name: glue_logs
experiment_name: gsm8k_redteam
# Many features are different for mode: online/offline. For eg
# 1) Print of logs happens on console for offline mode
# 2) LLM Queue gets instantiated only in online mode
mode: offline
# Full length description of the experiment. This would be logged.
description:
# content_moderation is optional. If you don't want to specify content_moderation,
# remove this entire field & its sub-fields.
content_moderation:
  # If content_severity crosses this threshold then the processing would be blocked
  content_severity_threshold: 2
  # Azure AI Content Safety (AACS) Endpoint
  aacs:
    subscription_id: "**"
    resource_group: "**"
    name: "vellm_aacs"
    location: "east us"
    sku_name: "S0"

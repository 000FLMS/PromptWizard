# Specify one or more prompt refinement technique to be used. If you specify more than one prompt refinement techniques,
# all these technique would run on same seed data. Result, iterations needed & cost incurred for each of these
# technique would be logged. And winning technique for each data instance and overall would be logged.

# Supported prompt refinement techniques: Basic, RecursiveEval, MedPrompt
# Uncomment techniques that you want to use
############################ Critique Task Description Start ############################
prompt_technique_name: "critique_n_refine"
# unique_model_id of model defined in llm_config.yaml
unique_model_id: az_gpt_4
# Number of iterations for conducting <mutation_rounds>  rounds of mutation of task description
# followed by refinement of instructions
mutate_refine_iterations: 5
# Number of rounds of mutation to be performed when generating different styles
mutation_rounds: 5
# Refine instruction post mutation
refine_instruction: true
# Number of iterations for refining task description and in context examples for few-shot
refine_task_eg_iterations: 5
# Number of variations of prompts to generate in given iteration
style_variation: 5
# Number of questions to be asked to LLM in a single batch, during training step
questions_batch_size: 5
# Number of batches of questions to correctly answered, for a prompt to be considered as performing good
min_correct_count: 3
# Max number of mini-batches on which we should evaluate our prompt
max_eval_batches: 6
# Number of top best performing prompts to be considered for next iterations
top_n: 1
# Description of task. e.g. "Solving a math problem", "Solving a biomedical MCQ based exam". More detailed the better.
task_description: "4 choice multiple-choice questions from various branches of knowledge. These questions spans from "
# Chain-of-thoughts based thinking style that's most aligned with your dataset. Default is "Lets think step by step."
base_instruction: "Lets think step by step."
# Instruction to LLM for specifying format in which your answer should be provided.
answer_format: "For each question present the reasoning followed correct option out of [{options}]."
# Number of samples from dataset, set aside as training data. In every iteration we would be drawing
# `questions_batch_size` examples from training data with replacement.
seen_set_size: 25
# Number of examples to be given for few shots
few_shot_count: 3
# Generate synthetic reasoning
generate_reasoning: true
# Generate description of an expert which can solve the task at hand
generate_expert_identity: true
# Generate keywords that describe the intent of the task
generate_intent_keywords: false
############################ Critique Task Description End ############################



